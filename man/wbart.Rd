% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/wbart.R
\name{wbart}
\alias{wbart}
\title{BART for continuous responses}
\usage{
wbart(
  x.train,
  y.train,
  x.test = matrix(0, 0, 0),
  sparse = FALSE,
  theta = 0,
  omega = 1,
  a = 0.5,
  b = 1,
  augment = FALSE,
  rho = NULL,
  xinfo = matrix(0, 0, 0),
  numcut = 100L,
  usequants = FALSE,
  cont = FALSE,
  rm.const = TRUE,
  grp = NULL,
  xnames = NULL,
  categorical.idx = NULL,
  power = 2,
  base = -1,
  split.prob = "polynomial",
  k = 2,
  sigmaf = NA,
  sigest = NA,
  sigdf = 3,
  sigquant = 0.9,
  lambda = NA,
  fmean = mean(y.train),
  w = rep(1, length(y.train)),
  ntree = 200L,
  ndpost = 1000L,
  nskip = 1000L,
  keepevery = 1L,
  nkeeptrain = ndpost,
  nkeeptest = ndpost,
  nkeeptestmean = ndpost,
  nkeeptreedraws = ndpost,
  printevery = 100L,
  transposed = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{x.train}{A matrix or a data frame of predictors values (for training) with each row corresponding to an 
observation and each column corresponding to a predictor. If a predictor is a factor with \eqn{q} levels in a data frame,
it is replaced with \eqn{q} dummy variables.}

\item{y.train}{A vector of continuous response values for training.}

\item{x.test}{A matrix or a data frame of predictors values for testing, which has the same structure as \code{x.train}.}

\item{sparse}{A Boolean argument indicating whether to replace the discrete uniform distribution for selecting a split 
variable with a categorical distribution whose event probabilities follow a Dirichlet distribution 
(see Linero (2018) for details).}

\item{theta}{Set \code{theta} parameter; zero means random.}

\item{omega}{Set \code{omega} parameter; zero means random.}

\item{a}{A sparse parameter of \eqn{Beta(a, b)} hyper-prior where \eqn{0.5<=a<=1}; a lower value induces more sparsity.}

\item{b}{A sparse parameter of \eqn{Beta(a, b)} hyper-prior; typically, \eqn{b=1}.}

\item{augment}{A Boolean argument indicating whether data augmentation is performed in the variable selection procedure 
of Linero (2018).}

\item{rho}{A sparse parameter; typically \eqn{\rho = p} where \eqn{p} is the number of predictors.}

\item{xinfo}{A matrix of cut-points with each row corresponding to a predictor and each column corresponding to a cut-point.
\code{xinfo=matrix(0.0,0,0)} indicates the cut-points are specified by BART.}

\item{numcut}{The number of possible cut-points; If a single number is given, this is used for all predictors; 
Otherwise a vector with length equal to \code{ncol(x.train)} is required, where the \eqn{i-}th element gives the number of 
cut-points for the \eqn{i-}th predictor in \code{x.train}. If \code{usequants=FALSE}, \code{numcut} equally spaced 
cut-points are used to cover the range of values in the corresponding column of \code{x.train}. 
If \code{usequants=TRUE}, then min(\code{numcut}, the number of unique values in the corresponding column of 
\code{x.train} - 1) cut-point values are used.}

\item{usequants}{A Boolean argument indicating how the cut-points in \code{xinfo} are generated; 
If \code{usequants=TRUE}, uniform quantiles are used for the cut-points; Otherwise, the cut-points are generated uniformly.}

\item{cont}{A Boolean argument indicating whether to assume all predictors are continuous.}

\item{rm.const}{A Boolean argument indicating whether to remove constant predictors.}

\item{grp}{A vector of group indices for predictors. For example, if \eqn{2} appears \eqn{3} times in \code{grp}, the second 
predictor of \code{x.train} is a categorical predictor with \eqn{3} levels. \code{grp} is required if \code{transposed=TRUE}.}

\item{xnames}{Column names of \code{x.train}. \code{xnames} is required if \code{transposed=TRUE}.}

\item{categorical.idx}{A vector of the column indices of categorical predictors in \code{x.train}. \code{categorical.idx} 
is required if \code{transposed=TRUE}.}

\item{power}{The power parameter of the polynomial splitting probability for the tree prior. Only used if 
\code{split.prob="polynomial"}.}

\item{base}{The base parameter of the polynomial splitting probability for the tree prior if \code{split.prob="polynomial"}; 
if \code{split.prob="exponential"}, the probability of splitting a node at depth \eqn{d} is \code{base}\eqn{^d}.}

\item{split.prob}{A string indicating what kind of splitting probability is used for the tree prior. If 
\code{split.prob="polynomial"}, the splitting probability in Chipman et al. (2010) is used; 
If \code{split.prob="exponential"}, the splitting probability in Ročková and Saha (2019) is used.}

\item{k}{The number of prior standard deviations that \eqn{E(Y|x) = f(x)} is away from \eqn{+/-.5}. The response 
(\code{y.train}) is internally scaled to the range from \eqn{-.5} to \eqn{.5}. The bigger \code{k} is, the more conservative 
the fitting will be.}

\item{sigmaf}{The standard deviation of \code{f}.}

\item{sigest}{A rough estimate of the error standard deviation, the square of which follows an inverse chi-squared prior. 
If \code{sigest=NA}, the rough estimate will be the usual least square estimator; Otherwise, the supplied value will be used.}

\item{sigdf}{The degrees of freedom for the error variance prior.}

\item{sigquant}{The quantile of the error variance prior, where \code{sigest} is placed. The closer the quantile is to 1, 
the more aggressive the fit will be.}

\item{lambda}{The scale parameter of the error variance prior.}

\item{fmean}{BART operates on \code{y.train} centered by \code{fmean}.}

\item{w}{A vector of weights which multiply the standard deviation.}

\item{ntree}{The number of trees in the ensemble.}

\item{ndpost}{The number of posterior samples returned.}

\item{nskip}{The number of posterior samples burned in.}

\item{keepevery}{Every \code{keepevery} posterior sample is kept to be returned to the user.}

\item{nkeeptrain}{The number of posterior samples returned for the train data.}

\item{nkeeptest}{The number of posterior samples returned for the test data.}

\item{nkeeptestmean}{The number of posterior samples returned for the test mean.}

\item{nkeeptreedraws}{The number of posterior samples returned for the tree draws.}

\item{printevery}{As the MCMC runs, a message is printed every \code{printevery} iterations.}

\item{transposed}{A Boolean argument indicating whether the matrices \code{x.train} and \code{x.test} are transposed.}

\item{verbose}{A Boolean argument indicating whether any messages are printed out.}
}
\value{
The function \code{wbart()} returns an object of type \code{wbart} which essentially is a list consisting of the 
following components.
\item{sigma}{A vector with \code{nskip+ndpost*keepevery} posterior samples of \eqn{\sigma}.}
\item{yhat.train.mean}{\code{colMeans(yhat.train)}.}
\item{yhat.train}{A matrix with \code{ndpost} rows and \code{nrow(x.train)} columns with each row corresponding to a draw 
\eqn{f*} from the posterior of \eqn{f} and each column corresponding to a training data point. The \eqn{(i,j)}-th element of
the matrix is \eqn{f*(x)} for the \eqn{i}-th kept draw of \eqn{f} and the \eqn{j}-th training data point. Burn-in posterior 
samples are dropped.}
\item{yhat.test.mean}{\code{colMeans(yhat.test)}.}
\item{yhat.test}{A matrix with \code{ndpost} rows and \code{nrow(x.test)} columns with each row corresponding to a draw 
\eqn{f*} from the posterior of \eqn{f} and each column corresponding to a test data point. The \eqn{(i,j)}-th element of
the matrix is \eqn{f*(x)} for the \eqn{i}-th kept draw of \eqn{f} and the \eqn{j}-th test data point. Burn-in posterior 
samples are dropped.}
\item{varcount}{A matrix with \code{ndpost} rows and \code{ncol(x.train)} columns with each row corresponding to a draw of 
the ensemble and each column corresponding to a predictor. The \eqn{(i,j)}-th element is the number of times that the \eqn{j}-
th predictor is used as a split variable in the \eqn{i}-th posterior sample.}
\item{varprob}{A matrix with \code{ndpost} rows and \code{ncol(x.train)} columns with each row corresponding to a draw of 
the ensemble and each column corresponding to a predictor. The \eqn{(i,j)}-th element is the split probability of the \eqn{j}-
th predictor in the \eqn{i}-th posterior sample. Only useful when DART is fit, i.e., \code{sparse=TRUE}.}
\item{treedraws}{A list containing the posterior samples of the ensembles (trees structures, split variables and split values);
Can be used for prediction.}
\item{proc.time}{The process time of running the function \code{wbart()}.}
\item{mu}{BART operates on \code{y.train} centered by \code{fmean}.}
\item{mr.vecs}{A list of \eqn{ncol(x.train)} sub-lists with each corresponding to a predictor; Each sub-list contains 
\code{ndpost} vectors with each vector containing the (birth) Metropolis ratios for splits using the predictor as the split 
variable in that posterior sample.}
\item{vip}{A vector of variable inclusion proportions (VIP) proposed in Chipman et al. (2010).}
\item{within.type.vip}{A vector of within-type VIPs proposed in Luo and Daniels (2021).}
\item{pvip}{A vector of marginal posterior variable inclusion probabilities (PVIP) proposed in Linero (2018); Only useful
when DART is fit, i.e., \code{sparse=TRUE}.}
\item{varprob.mean}{A vector of posterior split probabilities (PSP) proposed in Linero (2018); Only useful when DART is fit, 
i.e., \code{sparse=TRUE}.}
\item{mr.mean}{A matrix with \code{ndpost} rows and \code{ncol(x.train)} columns with each row corresponding to a draw of 
the ensemble and each column corresponding to a predictor. The \eqn{(i,j)}-th element is the average Metropolis acceptance ratio
per splitting rule using the \eqn{j}-th predictor in the \eqn{i}-th posterior sample.}
\item{mi}{A vector of Metropolis importance (MI) proposed in Luo and Daniels (2021).}
\item{rm.const}{A vector of indicators for the predictors (after dummification) used in BART; when the indicator is negative, 
it refers to remove that predictor.}
}
\description{
BART is a Bayesian approach to nonparametric function estimation and inference using a sum of trees.\cr
For a continuous response \eqn{y} and a \eqn{p-}dimensional vector of predictors \eqn{x = (x_1, ..., x_p)'}, 
BART models \eqn{y} and \eqn{x} using \deqn{y = f(x) + \epsilon,}
where \eqn{f} is a sum of Bayesian regression trees function and \eqn{\epsilon ~ N(0, \sigma^2)}.\cr
The function \code{wbart()} is inherited from the CRAN R package \strong{BART} (\emph{Copyright (C) 2017 Robert McCulloch 
and Rodney Sparapani}) and two modifications are made for the splitting probability and variable importance (see Details).
}
\details{
This function is inherited from \code{BART::wbart()}.
While the original features of \code{BART::wbart()} are preserved, two modifications are made.\cr
The first modification is to provide two types of split probability for BART. One split probability is proposed in
Chipman et al. (2010) and defined as \deqn{p(d) = \gamma * (1+d)^{-\beta},} where \eqn{d} is the depth of the node,
\eqn{\gamma \in (0,1)} and \eqn{\beta \in (0,\infty)}. The other split probability is proposed by Ročková and Saha (2019) 
and defined as \deqn{p(d) = \gamma^d,} where \eqn{\gamma \in (1/n, 1/2)}. BART with the second split probability is proved
to achieve the optimal posterior contraction.\cr
The second modification is to provide five types of variable importance measures (\code{vip}, \code{within.type.vip},
\code{pvip}, \code{varprob.mean} and \code{mi}) in the return object, for the sake of the existence of mixed-type predictors.
}
\examples{
 
## simulate data (Scenario C.M.1. in Luo and Daniels (2021))
set.seed(123)
data = mixone(500, 50, 1, FALSE)
## test wbart() function
res = wbart(data$X, data$Y, ntree=50, nskip=200, ndpost=500)
}
\references{
Chipman, H. A., George, E. I. and McCulloch, R. E. (2010). 
  "BART: Bayesian additive regression trees."
   \emph{Ann. Appl. Stat.} \strong{4} 266--298.
   
Linero, A. R. (2018). 
  "Bayesian regression trees for high-dimensional prediction and variable selection." 
  \emph{J. Amer. Statist. Assoc.} \strong{113} 626--636.

Luo, C. and Daniels, M.J. (2021)
  "Variable Selection Using Bayesian Additive Regression Trees."
  \emph{arXiv preprint arXiv:2112.13998}.
  
Ročková V, Saha E (2019). 
  “On theory for BART.” 
  \emph{In The 22nd International Conference on Artificial Intelligence and Statistics} (pp. 2839–2848). PMLR.
  
Sparapani, R., Spanbauer, C. and McCulloch, R. (2021).
  "Nonparametric machine learning and efficient computation with bayesian additive regression trees: the BART R package."
  \emph{J. Stat. Softw.} \strong{97} 1--66.
}
\seealso{
\code{\link{mc.wbart}}, \code{\link{pbart}} and \code{\link{pwbart}}.
}
\author{
Chuji Luo: \email{cjluo@ufl.edu} and Michael J. Daniels: \email{daniels@ufl.edu}.
}
